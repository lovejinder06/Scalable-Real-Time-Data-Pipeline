# ğŸš€ End-to-End Real-Time Data Engineering Pipeline

## ğŸ“‹ Project Overview

This project demonstrates a comprehensive **real-time data engineering pipeline** that orchestrates data ingestion, processing, and storage using enterprise-grade technologies. The pipeline simulates a real-world scenario where user data is continuously streamed, processed, and stored across multiple database systems.

### ğŸ¯ Business Use Case
Simulates a **user analytics platform** that captures real-time user events, processes them for insights, and stores data for both operational queries and analytical workloads.

## ğŸ—ï¸ Architecture

![System Architecture](https://github.com/airscholar/e2e-data-engineering/blob/main/Data%20engineering%20architecture.png)

### Data Flow
```
API (randomuser.me) â†’ Airflow â†’ PostgreSQL â†’ Kafka â†’ Spark â†’ Cassandra
                                     â†“
                              Control Center + Schema Registry
                                     â†“
                                  Docker
```

## ğŸ› ï¸ Technologies Used

| Component | Technology | Purpose |
|-----------|------------|---------|
| **Orchestration** | Apache Airflow | Workflow management and scheduling |
| **Streaming** | Apache Kafka + Zookeeper | Real-time data streaming |
| **Processing** | Apache Spark | Distributed data processing |
| **Storage (OLTP)** | PostgreSQL | Transactional database |
| **Storage (NoSQL)** | Cassandra | Scalable analytics database |
| **Containerization** | Docker | Environment management |
| **Monitoring** | Control Center + Schema Registry | Stream monitoring and schema management |

## ğŸ“ Skills Demonstrated

### Data Engineering Core
- âœ… **Real-time data streaming** with Kafka
- âœ… **ETL pipeline orchestration** with Airflow
- âœ… **Distributed data processing** with Spark
- âœ… **Multi-database architecture** (SQL + NoSQL)
- âœ… **Containerized deployment** with Docker

### Enterprise Patterns
- âœ… **Event-driven architecture**
- âœ… **Microservices design**
- âœ… **Schema management**
- âœ… **Monitoring and observability**
- âœ… **Scalable system design**

## ğŸš€ Quick Start

### Prerequisites
- Docker and Docker Compose
- Python 3.8+
- 8GB+ RAM recommended

### Setup Commands
```bash
# Clone the repository
git clone https://github.com/[YOUR_USERNAME]/e2e-realtime-data-pipeline.git
cd e2e-realtime-data-pipeline

# Start all services
docker-compose up -d

# Check service status
docker-compose ps

# View logs
docker-compose logs -f
```

### Access Points
- **Airflow UI**: http://localhost:8080 (admin/admin)
- **Kafka Control Center**: http://localhost:9021
- **PostgreSQL**: localhost:5432 (postgres/postgres)

## ğŸ“Š Project Components

### 1. Data Ingestion (Airflow)
- **DAG**: Scheduled data extraction from randomuser.me API
- **Frequency**: Configurable interval (default: every 1 minute)
- **Storage**: Initial data stored in PostgreSQL

### 2. Real-time Streaming (Kafka)
- **Producer**: Streams data from PostgreSQL to Kafka topics
- **Topics**: User events with configurable partitions
- **Schema Registry**: Manages data schemas and evolution

### 3. Stream Processing (Spark)
- **Structured Streaming**: Real-time data transformation
- **Processing**: Data cleansing, enrichment, and aggregation
- **Output**: Processed data written to Cassandra

### 4. Data Storage
- **PostgreSQL**: Operational data store
- **Cassandra**: Analytical data warehouse
- **Schema**: Optimized for both OLTP and OLAP workloads

## ğŸ“ Project Structure

```
â”œâ”€â”€ docker-compose.yml          # Container orchestration
â”œâ”€â”€ dags/                      # Airflow DAGs
â”‚   â””â”€â”€ kafka_stream.py        # Main pipeline DAG
â”œâ”€â”€ spark_stream.py            # Spark streaming application
â”œâ”€â”€ requirements.txt           # Python dependencies
â”œâ”€â”€ config/                    # Configuration files
â””â”€â”€ scripts/                   # Setup and utility scripts
```

## ğŸ”§ Configuration

### Environment Variables
```bash
# Kafka Configuration
KAFKA_BOOTSTRAP_SERVERS=broker:29092
KAFKA_TOPIC=users_created

# Database Configuration
POSTGRES_DB=airflow
POSTGRES_USER=airflow
POSTGRES_PASSWORD=airflow

# Cassandra Configuration
CASSANDRA_HOST=cassandra
CASSANDRA_KEYSPACE=spark_streams
```

## ğŸ“ˆ Performance Metrics

- **Throughput**: 1000+ messages/second
- **Latency**: Sub-second processing
- **Scalability**: Horizontally scalable components
- **Reliability**: Fault-tolerant with automatic recovery

## ğŸ§ª Testing & Validation

### Data Pipeline Testing
```bash
# Check data flow
docker exec -it cassandra cqlsh -e "SELECT COUNT(*) FROM spark_streams.created_users;"

# Monitor Kafka topics
docker exec -it broker kafka-console-consumer --topic users_created --from-beginning --bootstrap-server localhost:9092

# Validate Airflow DAGs
docker exec -it airflow-webserver airflow dags test kafka_stream_dag start_date
```

## ğŸš€ Production Considerations

### Scalability
- **Kafka**: Multiple brokers and partitions
- **Spark**: Cluster mode with multiple workers
- **Cassandra**: Multi-node cluster setup

### Monitoring
- **Metrics**: Custom metrics for throughput and latency
- **Alerting**: Integration with monitoring systems
- **Logging**: Centralized logging with ELK stack

### Security
- **Authentication**: SASL/SSL for Kafka
- **Authorization**: Role-based access control
- **Encryption**: Data encryption at rest and in transit

## ğŸ¯ Learning Outcomes

After completing this project, you will understand:

1. **Real-time streaming architecture** design principles
2. **Event-driven systems** and message queues
3. **Distributed computing** with Apache Spark
4. **Workflow orchestration** with Apache Airflow
5. **Multi-database strategies** for different workloads
6. **Containerization** and microservices deployment
7. **System monitoring** and observability

## ğŸ”— Related Projects

- **Batch Processing Pipeline**: ETL with Airflow and Spark
- **ML Pipeline**: Real-time model serving with Kafka
- **Analytics Dashboard**: Visualization with Apache Superset

## ğŸ“š Resources

- [Apache Kafka Documentation](https://kafka.apache.org/documentation/)
- [Apache Spark Streaming Guide](https://spark.apache.org/docs/latest/streaming-programming-guide.html)
- [Apache Airflow Best Practices](https://airflow.apache.org/docs/apache-airflow/stable/best-practices.html)
- [Cassandra Data Modeling](https://cassandra.apache.org/doc/latest/data_modeling/index.html)

## ğŸ“„ License

This project is licensed under the MIT License - see the [LICENSE](LICENSE) file for details.

## ğŸ¤ Contributing

1. Fork the repository
2. Create a feature branch (`git checkout -b feature/amazing-feature`)
3. Commit your changes (`git commit -m 'Add amazing feature'`)
4. Push to the branch (`git push origin feature/amazing-feature`)
5. Open a Pull Request

## ğŸ“ Contact

**Kartik Vadhawana**
- Email: kartikvadhwana7@gmail.com
- LinkedIn: [kartikvadhawana](https://linkedin.com/in/kartikvadhawana)
- GitHub: [VKartik-3](https://github.com/VKartik-3)

---

â­ **Star this repository if you found it helpful!**
